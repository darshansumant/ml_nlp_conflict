{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as tud\n",
    "from collections import Counter, defaultdict\n",
    "import operator\n",
    "import os, math\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seeds so the experiments can be replicated exactly\n",
    "seed = 30255\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('Data/data_prepr.csv')\n",
    "data = pd.read_csv('Data/data_prepr_final.csv')\n",
    "\n",
    "data = data[['tagged_rel', 'tagged_str']]\n",
    "\n",
    "data.tagged_str = data.tagged_str.str.replace('AGGRESOR', ' AGGRESOR ')\n",
    "data.tagged_str = data.tagged_str.str.replace('VICTIM', ' VICTIM ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34556, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    # reset index\n",
    "    data_idx = data.reset_index()\n",
    "    \n",
    "    # subsetting in .75 train and even random split the other\n",
    "    dev_test_size = round(len(data_idx)*0.25)\n",
    "    tr_df = data_idx[dev_test_size:]\n",
    "    dev_test_df = data_idx[:dev_test_size]\n",
    "    ran_idx = np.random.choice(dev_test_size, round(dev_test_size/2), replace=False)\n",
    "    dev_df = dev_test_df.iloc[ran_idx]\n",
    "    test_df = dev_test_df.iloc[~ran_idx]\n",
    "    \n",
    "    #shuffling training so it doesn't see similar cases one after the other \n",
    "    tr_df = tr_df.sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "    # make them lists\n",
    "    train_l = tr_df.values.tolist()\n",
    "    dev_l = dev_df.values.tolist()\n",
    "    test_l = test_df.values.tolist()\n",
    "    all_l = data_idx.values.tolist()\n",
    "\n",
    "    return train_l, dev_l, test_l, all_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list, dev_list, test_list, all_list = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE FOR PROCESSING THE TEXT FOR INPUTTING INTO NEURAL NETWORK \n",
    "VOCAB_SIZE = 5000\n",
    "\n",
    "def remove_stopwords(l):\n",
    "    STOP  = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than'] \n",
    "    l_clean = []\n",
    "    for i in STOP: \n",
    "        if i in l:\n",
    "            l_clean.append(i)\n",
    "    return l_clean \n",
    "\n",
    "def word_tokenize(s):\n",
    "    split_l = s.lower().replace('.', '').replace(',', '').replace(';', '').replace(':', '').replace('!', '').replace('?', '').replace('(', '').replace(')', '').split()\n",
    "    #clean_l = remove_stopwords(split_l)\n",
    "    #return clean_l\n",
    "    \n",
    "    return split_l\n",
    "\n",
    "class textModel:\n",
    "    def __init__(self, data):\n",
    "        # Vocabulary is a set that stores every word seen in the \n",
    "        # training data\n",
    "        self.vocab_counts = Counter([word for ix, label, content in data \n",
    "                              for word in word_tokenize(content)]\n",
    "                            ).most_common(VOCAB_SIZE-1)\n",
    "        # word to index mapping\n",
    "        self.word_to_idx = {k[0]: v+1 for v, k in \n",
    "                            enumerate(self.vocab_counts)}\n",
    "        # all the unknown words will be mapped to index 0\n",
    "        self.word_to_idx[\"UNK\"] = 0 \n",
    "        self.idx_to_word = {v:k for k, v in self.word_to_idx.items()}\n",
    "\n",
    "        self.vocab = set(self.word_to_idx.keys())\n",
    "        self.vocab_size = len(self.vocab) \n",
    "        \n",
    "        \n",
    "    def train_model(self, data):\n",
    "        '''\n",
    "        Train the model with the provided training data\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def classify(self, data):\n",
    "        '''\n",
    "        Classify the documents with the model\n",
    "        '''\n",
    "        raise \n",
    "        \n",
    "class labels:\n",
    "    def __init__(self, data):\n",
    "        # Vocabulary is a set that stores every word seen in the \n",
    "        # training data\n",
    "        self.label_counts = list(Counter([label for ix, label, content in data]).items())\n",
    "        # word to index mapping\n",
    "        self.label_to_idx = {k[0]: v+1 for v, k in \n",
    "                            enumerate(self.label_counts)}\n",
    "        # all the unknown words will be mapped to index 0\n",
    "        self.idx_to_label = {v:k for k, v in self.label_to_idx.items()}\n",
    "\n",
    "        self.labels = set(self.label_to_idx.keys())\n",
    "        self.labels_size = len(self.labels) + 1\n",
    "\n",
    "        \n",
    "class TextClassificationDataset(tud.Dataset):\n",
    "    '''\n",
    "    PyTorch provides a common dataset interface. \n",
    "    See https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "    The dataset encodes documents into indices. \n",
    "    With the PyTorch dataloader, you can easily get batched data for \n",
    "    training and evaluation. \n",
    "    '''\n",
    "    def __init__(self, data, labd):\n",
    "        \n",
    "        self.data = data\n",
    "        self.vocab_size = VOCAB_SIZE\n",
    "        self.tm = textModel(data)\n",
    "        self.labs = labd\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def wordToTensor(self, word):\n",
    "        tensor = torch.zeros(1, VOCAB_SIZE, dtype=torch.long)\n",
    "        tensor[0][tm.word_to_idx[word]] = 1\n",
    "        return tensor\n",
    "\n",
    "    def lineToTensor(self, textstr):\n",
    "        text = word_tokenize(textstr)\n",
    "        to_ix = tm.word_to_idx\n",
    "        tensor = autograd.Variable(torch.LongTensor([to_ix[w] if w in to_ix else to_ix['UNK']for w in text]))\n",
    "        return tensor\n",
    "    \n",
    "    def targetToTensor(self, lab): \n",
    "\n",
    "        label_to_ix = self.labs.label_to_idx\n",
    "        l = label_to_ix [lab]\n",
    "        target_idx = torch.tensor([[l]])\n",
    "        return target_idx\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        datum = self.data[idx]\n",
    "        label = self.targetToTensor(datum[1])\n",
    "        item = self.lineToTensor(datum[2])\n",
    "        \n",
    "        return item, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.autograd as autograd\n",
    "\n",
    "# https://github.com/claravania/lstm-pytorch/blob/master/model.py\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class LSTMRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, output_size):\n",
    "        super(LSTMRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,batch_first=True, bidirectional=False)\n",
    "        self.hidden2cat = nn.Linear(hidden_dim, output_size)\n",
    "        self.hidden = self.initHidden()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters())\n",
    "\n",
    "        # From HW\n",
    "        self.loss_fn = nn.NLLLoss(size_average=False)\n",
    "        self.num_epochs = 8\n",
    "        self.best_model = {}\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                    autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        #print(batch.size(-1))\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        # embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        \n",
    "        # ht is the last hidden state of the sequences\n",
    "        out_space = self.hidden2cat(self.hidden[0][-1])\n",
    "        #out_space = self.hidden2cat(lstm_out)\n",
    "        output = F.log_softmax(out_space)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "\n",
    "    def train_epoch(self, train_data, l):\n",
    "        '''\n",
    "        '''\n",
    "        data = TextClassificationDataset(train_data, l)\n",
    "        data = tud.DataLoader(data, batch_size = 1, shuffle = True)\n",
    "        # Forward pass\n",
    "            \n",
    "        for sent, targets in data: \n",
    "            self.zero_grad() \n",
    "            self.hidden = self.initHidden()\n",
    "            pred = self.forward(sent)\n",
    "            \n",
    "            # Calculate loss with prediction and target labels\n",
    "            loss = self.loss_fn(pred, targets[0][0])\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Apply optimizer step. \n",
    "            self.optimizer.step()\n",
    "            \n",
    "    def classify(self, val_data, l):\n",
    "        '''\n",
    "        This function classifies/predicts documents into their categories. \n",
    "        '''\n",
    "        classif = []\n",
    "        cuml_loss = 0\n",
    "        \n",
    "        data = TextClassificationDataset(val_data, l)\n",
    "        data = tud.DataLoader(data, batch_size = 1, shuffle = True)\n",
    "        with torch.no_grad():\n",
    "            for sent, targets in data: \n",
    "                self.hidden = self.initHidden()\n",
    "                pred = self.forward(sent)\n",
    "\n",
    "                # Calculate loss with prediction and target labels\n",
    "                loss = self.loss_fn(pred, targets[0][0])\n",
    "                cuml_loss += loss\n",
    "\n",
    "                # fingin the max label for accuracy\n",
    "                pred_idx = ((torch.topk(pred, 1)[1])[0]).item()\n",
    "                \n",
    "                pred_rel = l.idx_to_label[pred_idx]\n",
    "\n",
    "                # storing into output\n",
    "                targ_rel = l.idx_to_label[(targets[0][0].item())]\n",
    "                classif.append((targ_rel, sent, pred_rel))\n",
    "\n",
    "        return classif, cuml_loss            \n",
    "                \n",
    "                \n",
    "    def evaluate_classifier_accuracy(self, classif_dat):\n",
    "        '''\n",
    "        This function evaluates the data with the current model. \n",
    "        data contains both documents and labels. \n",
    "        It calls classify() to make predictions, \n",
    "        and compares with the correct labels to return \n",
    "        the model accuracy on \"data\". \n",
    "        '''\n",
    "        num_errors = 0\n",
    "        # compares target to pred and calculates error rate\n",
    "        for target, note, pred in classif_dat:\n",
    "\n",
    "            if pred != target: \n",
    "                num_errors += 1\n",
    "        pred_error = num_errors/len(classif_dat) \n",
    "        \n",
    "        return (1 - pred_error)\n",
    "    \n",
    "    \n",
    "    def train_model(self, train_data, dev_data, l):\n",
    "        \"\"\"\n",
    "        This function processes the entire training set for multiple epochs.\n",
    "        After each training epoch, evaluate your model on the DEV set. \n",
    "        Save the best performing model on the DEV set to best_model\n",
    "        \"\"\"          \n",
    "\n",
    "        best_acc = 0\n",
    "        for i in range(self.num_epochs):\n",
    "            self.train_epoch(train_data, l)\n",
    "            classif, cuml_loss = self.classify(dev_data, l)\n",
    "            acc = self.evaluate_classifier_accuracy(classif)\n",
    "            print('epoch: {}; accuracy: {}; NLLLoss: {}'.format(i, acc, cuml_loss))\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc \n",
    "                self.best_model['model'] = copy.deepcopy(self)\n",
    "                self.best_model['acc'] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-54-0a4c8599d7ba>, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-54-0a4c8599d7ba>\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    def forward(self, sentence):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.autograd as autograd\n",
    "\n",
    "# https://github.com/claravania/lstm-pytorch/blob/master/model.py\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class BiDirLSTMRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, output_size):\n",
    "        super(BiDirLSTMRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,batch_first=True, bidirectional=True)\n",
    "        self.hidden2cat = nn.Linear(hidden_dim, output_size)\n",
    "        self.hidden = self.initHidden()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters())\n",
    "\n",
    "        # From HW\n",
    "        self.loss_fn = nn.NLLLoss(size_average=False)\n",
    "        self.num_epochs = 15\n",
    "        self.best_model = {}\n",
    "        \n",
    "    def initHidden(self):\n",
    "        '''\n",
    "        Sets the size and expected output for the hidden layer\n",
    "        '''\n",
    "        return (autograd.Variable(torch.randn(2, 1, self.hidden_dim // 2)),\n",
    "                autograd.Variable(torch.randn(2, 1, self.hidden_dim // 2)))\n",
    "      \n",
    "    def forward(self, sentence):\n",
    "        '''\n",
    "        Forward pass\n",
    "        '''\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        \n",
    "        # self.hidden[0][-1] is the last hidden state of the sequences\n",
    "        out_space = self.hidden2cat(torch.cat((self.hidden[0][0], self.hidden[0][1]), 0)) # MODIFICATION TO VERIFY W AMITABH - TRY ADDING AND TRY CONCATENATING\n",
    "        \n",
    "        # softmax for output layer. \n",
    "        output = F.log_softmax(out_space )\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def train_epoch(self, train_data, l):\n",
    "        '''\n",
    "        Trains an epoch. \n",
    "        '''\n",
    "        data = TextClassificationDataset(train_data, l)\n",
    "        data = tud.DataLoader(data, batch_size = 1, shuffle = True)\n",
    "        # Forward pass\n",
    "            \n",
    "        for sent, targets in data: \n",
    "            self.zero_grad() \n",
    "            self.hidden = self.initHidden()\n",
    "            pred = self.forward(sent)\n",
    "            \n",
    "            # Calculate loss with prediction and target labels\n",
    "\n",
    "            loss = self.loss_fn(pred, targets[0][0])\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Apply optimizer step. \n",
    "            self.optimizer.step()\n",
    "            \n",
    "    def classify(self, val_data, l):\n",
    "        '''\n",
    "        This function classifies/predicts documents into their categories. \n",
    "        '''\n",
    "        classif = []\n",
    "        cuml_loss = 0\n",
    "        \n",
    "        data = TextClassificationDataset(val_data, l)\n",
    "        data = tud.DataLoader(data, batch_size = 1, shuffle = True)\n",
    "        with torch.no_grad():\n",
    "            for sent, targets in data: \n",
    "                self.hidden = self.initHidden()\n",
    "                pred = self.forward(sent)\n",
    "\n",
    "                # Calculate loss with prediction and target labels\n",
    "                loss = self.loss_fn(pred, targets[0][0])\n",
    "                cuml_loss += loss\n",
    "\n",
    "                # fingin the max label for accuracy\n",
    "                pred_idx = ((torch.topk(pred, 1)[1])[0]).item()\n",
    "                \n",
    "                pred_rel = l.idx_to_label[pred_idx]\n",
    "\n",
    "                # storing into output\n",
    "                targ_rel = l.idx_to_label[(targets[0][0].item())]\n",
    "                classif.append((targ_rel, sent, pred_rel))\n",
    "\n",
    "        return classif, cuml_loss            \n",
    "                \n",
    "                \n",
    "    def evaluate_classifier_accuracy(self, classif_dat):\n",
    "        '''\n",
    "        This function gets the accuracy of classified data. \n",
    "        '''\n",
    "        num_errors = 0\n",
    "        # compares target to pred and calculates error rate\n",
    "        for target, note, pred in classif_dat:\n",
    "            if pred != target: \n",
    "                num_errors += 1\n",
    "        pred_error = num_errors/len(classif_dat) \n",
    "        \n",
    "        return (1 - pred_error)\n",
    "    \n",
    "    \n",
    "    def train_model(self, train_data, dev_data, l):\n",
    "        \"\"\"\n",
    "        This function processes the entire training set for multiple epochs.\n",
    "        After each training epoch, evaluate your model on the DEV set. \n",
    "        Save the best performing model on the DEV set to best_model\n",
    "        \"\"\"          \n",
    "\n",
    "        best_acc = 0\n",
    "        for i in range(self.num_epochs):\n",
    "            self.train_epoch(train_data, l)\n",
    "            classif, cuml_loss = self.classify(dev_data, l)\n",
    "            acc = self.evaluate_classifier_accuracy(classif)\n",
    "            print('epoch: {}; accuracy: {}; NLLLoss: {}'.format(i, acc, cuml_loss))\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc \n",
    "                self.best_model['model'] = copy.deepcopy(self)\n",
    "                self.best_model['acc'] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMB_DIM = 10\n",
    "HID_DIM = 40\n",
    "labs = labels(all_list)\n",
    "tm = textModel(small_train)\n",
    "tm.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crismacg/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0; accuracy: 0.6238425925925926; NLLLoss: 6555.48486328125\n",
      "epoch: 1; accuracy: 0.7194444444444444; NLLLoss: 4942.01611328125\n",
      "epoch: 2; accuracy: 0.7328703703703704; NLLLoss: 4683.599609375\n",
      "epoch: 3; accuracy: 0.7347222222222223; NLLLoss: 4736.33349609375\n",
      "epoch: 4; accuracy: 0.7361111111111112; NLLLoss: 4741.58251953125\n",
      "epoch: 5; accuracy: 0.731712962962963; NLLLoss: 4796.4287109375\n",
      "epoch: 6; accuracy: 0.7395833333333333; NLLLoss: 4820.32568359375\n",
      "epoch: 7; accuracy: 0.7328703703703704; NLLLoss: 4873.9833984375\n",
      "epoch: 8; accuracy: 0.7293981481481482; NLLLoss: 5071.0595703125\n",
      "epoch: 9; accuracy: 0.7268518518518519; NLLLoss: 5071.08349609375\n",
      "epoch: 10; accuracy: 0.7256944444444444; NLLLoss: 5168.14697265625\n",
      "epoch: 11; accuracy: 0.7238425925925926; NLLLoss: 5261.71826171875\n",
      "epoch: 12; accuracy: 0.7199074074074074; NLLLoss: 5292.1435546875\n",
      "epoch: 13; accuracy: 0.725; NLLLoss: 5414.49462890625\n",
      "epoch: 14; accuracy: 0.7231481481481481; NLLLoss: 5343.77001953125\n",
      "epoch: 15; accuracy: 0.7222222222222222; NLLLoss: 5543.7490234375\n",
      "epoch: 16; accuracy: 0.7173611111111111; NLLLoss: 5610.62841796875\n",
      "epoch: 17; accuracy: 0.7247685185185185; NLLLoss: 5535.74462890625\n",
      "epoch: 18; accuracy: 0.7162037037037037; NLLLoss: 5640.14892578125\n",
      "epoch: 19; accuracy: 0.7138888888888889; NLLLoss: 5796.453125\n"
     ]
    }
   ],
   "source": [
    "model_lstm = LSTMRNN(EMB_DIM, HID_DIM, tm.vocab_size, labs.labels_size)\n",
    "\n",
    "model_lstm.train_model(train_list, dev_list, labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crismacg/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0; accuracy: 0.6287037037037038; NLLLoss: 6352.1689453125\n",
      "epoch: 1; accuracy: 0.7027777777777777; NLLLoss: 5298.0634765625\n",
      "epoch: 2; accuracy: 0.7159722222222222; NLLLoss: 5062.89013671875\n",
      "epoch: 3; accuracy: 0.7247685185185185; NLLLoss: 4995.65087890625\n",
      "epoch: 4; accuracy: 0.7224537037037038; NLLLoss: 4906.1376953125\n",
      "epoch: 5; accuracy: 0.7238425925925926; NLLLoss: 4966.78271484375\n",
      "epoch: 6; accuracy: 0.7268518518518519; NLLLoss: 4904.794921875\n",
      "epoch: 7; accuracy: 0.724537037037037; NLLLoss: 4979.283203125\n",
      "epoch: 8; accuracy: 0.7287037037037036; NLLLoss: 5054.5546875\n",
      "epoch: 9; accuracy: 0.7243055555555555; NLLLoss: 5032.3671875\n",
      "epoch: 10; accuracy: 0.73125; NLLLoss: 5053.33935546875\n",
      "epoch: 11; accuracy: 0.7256944444444444; NLLLoss: 5076.46728515625\n",
      "epoch: 12; accuracy: 0.7182870370370371; NLLLoss: 5215.724609375\n",
      "epoch: 13; accuracy: 0.7261574074074074; NLLLoss: 5149.39404296875\n",
      "epoch: 14; accuracy: 0.7261574074074074; NLLLoss: 5105.72412109375\n"
     ]
    }
   ],
   "source": [
    "# Running 50 epochs of bidirectional LSTM \n",
    "model_bd = BiDirLSTMRNN(EMB_DIM, HID_DIM, tm.vocab_size, labs.labels_size)\n",
    "\n",
    "model_bd.train_model(train_list, dev_list, labs)\n",
    "# ----- code for tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crismacg/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------- Test LSTM \n",
    "classif, loss = model_lstm.best_model['model'].classify(test_list, labs)\n",
    "print('accuracy according to validation set {}'.format(model_lstm.best_model['acc']))\n",
    "print('accuracy according to test set {}'.format(model_lstm.evaluate_classifier_accuracy(classif))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crismacg/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------ Test Bidirectional LSTM \n",
    "classif, loss = model_bd.best_model['model'].classify(test_list, labs)\n",
    "print('accuracy according to validation set {}'.format(model_bd.best_model['acc']))\n",
    "print('accuracy according to test set {}'.format(model_bd.evaluate_classifier_accuracy(classif))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_models = LSTMRNN(EMB_DIM, HID_DIM, tm.vocab_size, labs.labels_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crismacg/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0; accuracy: 0.7108796296296296; NLLLoss: 5860.40673828125\n",
      "epoch: 1; accuracy: 0.7083333333333333; NLLLoss: 5993.873046875\n",
      "epoch: 2; accuracy: 0.7118055555555556; NLLLoss: 5869.85791015625\n",
      "epoch: 3; accuracy: 0.7074074074074075; NLLLoss: 5872.5068359375\n",
      "epoch: 4; accuracy: 0.7078703703703704; NLLLoss: 5873.8330078125\n",
      "epoch: 5; accuracy: 0.7125; NLLLoss: 5787.69580078125\n",
      "epoch: 6; accuracy: 0.7076388888888889; NLLLoss: 5981.404296875\n",
      "epoch: 7; accuracy: 0.7064814814814815; NLLLoss: 5915.35595703125\n",
      "Iteration 1 completed\n",
      "epoch: 0; accuracy: 0.7141203703703703; NLLLoss: 5916.125\n",
      "epoch: 1; accuracy: 0.712962962962963; NLLLoss: 5951.59033203125\n",
      "epoch: 2; accuracy: 0.7111111111111111; NLLLoss: 5972.73828125\n",
      "epoch: 3; accuracy: 0.7111111111111111; NLLLoss: 5996.56201171875\n",
      "epoch: 4; accuracy: 0.7097222222222221; NLLLoss: 6031.03076171875\n",
      "epoch: 5; accuracy: 0.7074074074074075; NLLLoss: 6053.3623046875\n",
      "epoch: 6; accuracy: 0.7101851851851853; NLLLoss: 6060.77734375\n",
      "epoch: 7; accuracy: 0.7092592592592593; NLLLoss: 6103.48681640625\n",
      "Iteration 2 completed\n",
      "epoch: 0; accuracy: 0.5678240740740741; NLLLoss: 8887.1201171875\n",
      "epoch: 1; accuracy: 0.4722222222222222; NLLLoss: 10689.166015625\n",
      "epoch: 2; accuracy: 0.469212962962963; NLLLoss: 11242.3369140625\n",
      "epoch: 3; accuracy: 0.3324074074074074; NLLLoss: 11712.505859375\n",
      "epoch: 4; accuracy: 0.3324074074074074; NLLLoss: 12269.744140625\n",
      "epoch: 5; accuracy: 0.3324074074074074; NLLLoss: 13175.88671875\n",
      "epoch: 6; accuracy: 0.3324074074074074; NLLLoss: 13429.3837890625\n",
      "epoch: 7; accuracy: 0.3324074074074074; NLLLoss: 13551.1337890625\n",
      "Iteration 3 completed\n",
      "epoch: 0; accuracy: 0.6960648148148147; NLLLoss: 5404.84033203125\n",
      "epoch: 1; accuracy: 0.6983796296296296; NLLLoss: 5720.7333984375\n",
      "epoch: 2; accuracy: 0.7104166666666667; NLLLoss: 5511.447265625\n",
      "epoch: 3; accuracy: 0.7122685185185185; NLLLoss: 5541.6494140625\n",
      "epoch: 4; accuracy: 0.6520833333333333; NLLLoss: 5962.58154296875\n",
      "epoch: 5; accuracy: 0.6833333333333333; NLLLoss: 5787.125\n",
      "epoch: 6; accuracy: 0.6648148148148147; NLLLoss: 5785.97119140625\n",
      "epoch: 7; accuracy: 0.6828703703703703; NLLLoss: 5415.34326171875\n",
      "Iteration 4 completed\n",
      "epoch: 0; accuracy: 0.3324074074074074; NLLLoss: 13831.662109375\n",
      "epoch: 1; accuracy: 0.3324074074074074; NLLLoss: 15342.24609375\n",
      "epoch: 2; accuracy: 0.3324074074074074; NLLLoss: 14657.9326171875\n",
      "epoch: 3; accuracy: 0.3324074074074074; NLLLoss: 15535.9423828125\n",
      "epoch: 4; accuracy: 0.3324074074074074; NLLLoss: 14673.75390625\n",
      "epoch: 5; accuracy: 0.3324074074074074; NLLLoss: 15703.0859375\n",
      "epoch: 6; accuracy: 0.3324074074074074; NLLLoss: 13218.310546875\n",
      "epoch: 7; accuracy: 0.3324074074074074; NLLLoss: 14505.0185546875\n",
      "Iteration 5 completed\n",
      "epoch: 0; accuracy: 0.3324074074074074; NLLLoss: 12694.2939453125\n",
      "epoch: 1; accuracy: 0.3324074074074074; NLLLoss: 12482.8017578125\n"
     ]
    }
   ],
   "source": [
    "# TUNING THE MODEL \n",
    "# Option 1: \n",
    "results = {}\n",
    "\n",
    "optimizers = ['Adam', 'SGD']\n",
    "lrates = [0.0001, 0.01]\n",
    "regularization = [0, .3]\n",
    "epochs = [8]\n",
    "count = 0\n",
    "for optim in optimizers:\n",
    "    results[optim] = {}\n",
    "    for rate in lrates: \n",
    "        results[optim][rate] = {}\n",
    "        for regul in regularization: \n",
    "            results[optim][rate][regul] = {}\n",
    "            for n_epochs in epochs: \n",
    "                count += 1\n",
    "                tuning_models.num_epochs = n_epochs\n",
    "                tuning_models.train_model(train_list, dev_list, labs)\n",
    "                if optim == 'Adam': \n",
    "                    tuning_models.optimizer = torch.optim.Adam(tuning_models.parameters(), lr = rate, weight_decay = regul)\n",
    "                if optim == 'SGD': \n",
    "                    tuning_models.optimizer = torch.optim.SGD(tuning_models.parameters(), lr = rate, weight_decay = regul)\n",
    "                \n",
    "                results[optim][rate][regul]['best_m'] = tuning_models.best_model['model']\n",
    "                results[optim][rate][regul]['acc'] = tuning_models.best_model['acc']\n",
    "                print('Iteration {} completed'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â SELECTING BEST MODEL \n",
    "\n",
    "best = {}\n",
    "best['best_acc'] = 0\n",
    "best['best_mod'] = None\n",
    "for optim in optimizers:\n",
    "    for rate in lrates: \n",
    "        for regul in regularization: \n",
    "            for n_epochs in epochs: \n",
    "                if results[optim][rate][regul]['acc'] > best['best_acc']:                     \n",
    "                    best['best_acc'] = results[optim][rate][regul]['acc']\n",
    "                    print(results[optim][rate][regul])\n",
    "                    best['best_mod'] = results[optim][rate][regul]['best_m']\n",
    "                    best['n_epochs'] = n_epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDENTIFY WHERE WE ARE MAKING MORE MISTAKES \n",
    "results = pd.DataFrame(classif, usecols = ['tar', 'sent', 'pred'])\n",
    "results['correct'] = results.apply(lambda x: 1 if x['tar'] == x['pred'] else 0)\n",
    "results.groupby('tar')['correct'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Where are we making mistakes \n",
    "results = pd.DataFrame(classif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
